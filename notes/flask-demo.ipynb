{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flask demo\n",
    "\n",
    "Flask will be our web service tool.\n",
    "\n",
    "Demo `ping.py`\n",
    "\n",
    "```python\n",
    "## ping.py\n",
    "from flask import Flask\n",
    "app = Flask('ping')\n",
    "\n",
    "@app.route('/ping', methods=['GET'])\n",
    "def ping():\n",
    "    return 'PONG'\n",
    "\n",
    "\n",
    "if __name__  == '__main__':\n",
    "#     # only run if called from CLI as a script\n",
    "#     # or with python -m\n",
    "    app.run(debug=True, host='0.0.0.0', port=9222)\n",
    "```\n",
    "\n",
    "Once running, run this in the terminal:\n",
    "\n",
    "```bash\n",
    "curl http://0.0.0.0:9222/ping\n",
    "```\n",
    "\n",
    "`curl` sends a `GET` request, and because we specified `/ping`, it will run the `ping()` routine and return `PONG` in our terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## http methods\n",
    "\n",
    "These are the methods by which the HyperText Transfer Protocol communicates between server and client. \n",
    "\n",
    "* GET\n",
    "* POST\n",
    "* PUT\n",
    "* HEAD\n",
    "* DELETE\n",
    "* PATCH\n",
    "* OPTIONS\n",
    "* CONNECT\n",
    "* TRACE\n",
    "\n",
    "For example, a client may send an HTTP request to server, and depending on the method, the server may reply with a different response. The response contains the request status, and potentially the requested content.\n",
    "\n",
    "`GET` requests data from source. `POST` sends data to server. They are the two most common methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving a model with flask\n",
    "\n",
    "Our imports. The shebang `#!/usr/bin/env python` is a little different here, in that we use the `env` command to find the path to `python` executable. Having multiple versions of python that we do, in our conda envs, this is very useful, as it will use the current active conda env's python version.\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "## predict.py\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "```\n",
    "\n",
    "Where we make our prediction\n",
    "```python\n",
    "def predict_single(customer, dv, model):\n",
    "    X = dv.transform([customer])\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "    return y_pred[0]\n",
    "```\n",
    "\n",
    "Deploying the serialized, pre-trained model\n",
    "```python\n",
    "with open('churn-model.bin', 'rb') as f_in:\n",
    "    dv, model = pickle.load(f_in)\n",
    "```\n",
    "\n",
    "Running the web service. Notice the `POST` method: the request must come with a json of the customer features; the client is POSTing the data to the server. In turn the web service returns with a json response containing the prediction\n",
    "```python\n",
    "app = Flask('churn')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    customer = request.get_json()\n",
    "\n",
    "    prediction = predict_single(customer, dv, model)\n",
    "    churn = prediction >= 0.5\n",
    "    \n",
    "    result = {\n",
    "        ## converts numpy types to python types\n",
    "        ## otherwise json cannot serialize the values.\n",
    "        'churn_probability': float(prediction),\n",
    "        'churn': bool(churn),\n",
    "    }\n",
    "\n",
    "    return jsonify(result)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, host='0.0.0.0', port=9696)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development vs Production Server\n",
    "\n",
    "Running the `ping.py` script above will return a warning that this is a development server. Production requires `WSGI server` instead.\n",
    "\n",
    "In short, plain `flask` is for testing only. `gunicorn` is an example of WSGI server. What is **WSGI (Web Server Gateway Interface)**? It is a specification describing how web server communicates with web apps, and how web apps can chain together to process a request. The idea here is that it *abstracts away whichever web framework or web server developers want to work with*; if it follows the WSGI spec, they can communicate, between server and app. \n",
    "\n",
    "*WSGI server acts as the interface between app and server*. [See python PEP-3333](https://peps.python.org/pep-3333/)\n",
    "\n",
    "To use `gunicorn` for the prediction app above:\n",
    "\n",
    "```bash\n",
    "# bash\n",
    "gunicorn --bind 0.0.0.0:9696 predict:app\n",
    "```\n",
    "\n",
    "And that's it. `predict` is the name of the file - `predict.py`, and `app` is our `Flask` object\n",
    "\n",
    "### Windows\n",
    "\n",
    "`gunicorn` does not work on windows - use `mod_wsgi` or `waitress-serve` instead:\n",
    "\n",
    "```shell\n",
    "# shell\n",
    "waitress-serve --listen=0.0.0.0:9696 predict:app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2759d39a0c80892509c071ef2701a81133b36493b3db182a1c8eaae81290a314"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mlops')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
